# Complete hyperparameter optimization configuration
# This file documents every option. See optimize.minimal.yaml for a starting template.
# Run with: basil optimize configs/optimize.full.yaml

# Output directory for optimization results and best model
output_dir: ./output/tune_results

# ============================================================================
# BASE CONFIGURATION (Fixed parameters)
# ============================================================================
# These parameters will NOT be tuned - they remain constant across all trials.
# Any parameter not in search_space uses these values.

base_config:
  data:
    # Required: Path to the .npy dataset file
    path: embeddings.npy

    # Batch size for training
    # Keep > 2048 for healthy codebook utilization
    batch_size: 4096

    # Number of DataLoader workers
    num_workers: 4

    # Fraction of dataset for validation
    val_set_size: 0.05

  model:
    # Required: Dimension of input embeddings (must match your .npy file)
    input_dim: 768

  train:
    # Epochs per trial (ASHA will early-stop bad trials before this)
    epochs: 20

    # Random seed for reproducibility
    seed: 42

    # Device: 'auto', 'cuda', 'mps', 'cpu'
    device: auto

    # Enable mixed precision training
    use_amp: true

    # Log metrics every N steps
    log_interval: 100

    # Disable intermediate checkpoint saves during tuning (saves disk space)
    save_interval: 0


# ============================================================================
# SEARCH SPACE (Parameters to optimize)
# ============================================================================
# Supported types:
#   - uniform:    Continuous uniform distribution (low, high)
#   - loguniform: Log-uniform distribution (low, high) - good for learning rates
#   - randint:    Random integer [low, high) - exclusive upper bound
#   - choice:     Choose from a list of discrete values
#   - quniform:   Quantized uniform (low, high, q) - rounds to nearest q

search_space:
  model:
    # Architecture: List of encoder layer dimensions
    # Decoder will be the inverse. Last dim is the latent/quantization dim.
    encoder_dims:
      type: choice
      values:
        - [512, 256, 64]
        - [512, 256, 128, 64]
        - [768, 384, 192, 96]
        - [768, 384, 192, 96, 48]
        - [768, 512, 256, 128, 64, 32]

    # Number of entries in each codebook level
    codebook_size:
      type: choice
      values: [256, 384, 512, 768, 1024]

    # Depth of residual quantization (number of semantic ID tokens)
    num_levels:
      type: randint
      low: 2
      high: 5  # exclusive, so 2-4 levels

    # Dropout probability for regularization
    dropout:
      type: uniform
      low: 0.0
      high: 0.2

    # Use hierarchical codebook sizes (each level half the previous)
    use_hierarchical:
      type: choice
      values: [true, false]

  train:
    # Peak learning rate (log scale is better for LR search)
    lr:
      type: loguniform
      low: 1.0e-4
      high: 5.0e-3

    # VQ-VAE commitment loss weight
    commitment_beta:
      type: loguniform
      low: 0.005
      high: 0.5

    # EMA decay factor for codebook updates
    ema_decay:
      type: uniform
      low: 0.95
      high: 0.999

    # Use stochastic sampling during training
    stochastic_sampling:
      type: choice
      values: [true, false]

    # Temperature for stochastic sampling (higher = more random)
    stochastic_temperature:
      type: uniform
      low: 0.3
      high: 1.0

    # Enable progressive masking for hierarchical independence
    use_progressive_masking:
      type: choice
      values: [true, false]

    # Probability of truncating to fewer quantization levels
    progressive_mask_prob:
      type: uniform
      low: 0.1
      high: 0.5


# ============================================================================
# OPTIMIZATION SETTINGS
# ============================================================================

optimization:
  # Number of different configurations to try
  # More samples = better coverage but longer runtime
  num_samples: 30

  # Metric to optimize (from validation metrics)
  # Options: val_total_loss, val_recon_loss, val_vq_loss, val_cos_sim, val_ppl_0, etc.
  metric: val_cos_sim

  # Optimization direction: "max" or "min"
  mode: max

  # Maximum concurrent trials
  # Set based on your GPU count. 1 = sequential trials.
  max_concurrent: 1

  # Early stopping scheduler: "asha" or "none"
  # ASHA (Async Successive Halving) aggressively stops underperforming trials
  scheduler: asha

  # ASHA settings (only used if scheduler: asha)

  # Minimum epochs before a trial can be stopped
  grace_period: 3

  # Trials are halted if in bottom 1/reduction_factor
  # 3 means bottom 1/3 of trials are stopped at each rung
  reduction_factor: 3

